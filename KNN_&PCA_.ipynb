{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "QUES 1:What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "ANS:K-Nearest Neighbors (KNN):\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised, non-parametric, and instance-based machine learning algorithm. It does not build an explicit model during training. Instead, it stores the entire training dataset and makes predictions based on the similarity (distance) between data points.\n",
        "\n",
        "How KNN Works (General Steps):\n",
        "\n",
        "Choose the number of neighbors (K).\n",
        "\n",
        "Calculate the distance between the new data point and all training points\n",
        "(commonly Euclidean distance).\n",
        "\n",
        "Select the K nearest neighbors.\n",
        "\n",
        "Make a prediction based on these neighbors.\n",
        "\n",
        "KNN for Classification:\n",
        "\n",
        "In classification, KNN predicts the class label of a new data point by majority voting.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compute distances between the new point and all training samples.\n",
        "\n",
        "Select the K closest data points.\n",
        "\n",
        "Count the class labels among these K neighbors.\n",
        "\n",
        "Assign the class with the highest frequency.\n",
        "\n",
        "Example:\n",
        "If K = 5 and among the 5 neighbors,\n",
        "\n",
        "3 belong to Class A\n",
        "\n",
        "2 belong to Class B\n",
        "\n",
        "➡ The new data point is classified as Class A.\n",
        "\n",
        "KNN for Regression:\n",
        "\n",
        "In regression, KNN predicts a continuous value by averaging the values of the K nearest neighbors.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compute distances to all training points.\n",
        "\n",
        "Select the K nearest neighbors.\n",
        "\n",
        "Take the mean (or weighted mean) of their target values.\n",
        "\n",
        "Assign this value as the prediction.\n",
        "\n",
        "Example:\n",
        "If K = 3 and neighbor values are: 50, 55, 60\n",
        "\n",
        "➡ Predicted value = (50 + 55 + 60) / 3 = 55"
      ],
      "metadata": {
        "id": "fWGbn99tvtJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "\n",
        "Ans:The Curse of Dimensionality: refers to the problems that arise when working with high-dimensional data (data with many features). As the number of dimensions increases, the data becomes sparse, and many machine learning algorithms struggle to learn meaningful patterns.\n",
        "\n",
        "How the Curse of Dimensionality Affects KNN\n",
        "\n",
        "K-Nearest Neighbors (KNN) relies entirely on distance calculations to find similar data points. In high-dimensional spaces, this becomes problematic.\n",
        "\n",
        "1. Distance Becomes Less Meaningful\n",
        "\n",
        "In high dimensions, the distance between the nearest and farthest data points becomes almost the same.\n",
        "\n",
        "As a result, KNN cannot clearly distinguish between close and distant neighbors.\n",
        "\n",
        "➡ This reduces the reliability of neighbor selection.\n",
        "\n",
        "2. Increased Sparsity of Data\n",
        "\n",
        "With more features, data points spread out and become sparse.\n",
        "\n",
        "Each data point has fewer nearby neighbors.\n",
        "\n",
        "➡ KNN predictions become unstable and less accurate.\n",
        "\n",
        "3. Higher Computational Cost\n",
        "\n",
        "KNN calculates distance from a test point to all training points.\n",
        "\n",
        "More dimensions mean more distance calculations.\n",
        "\n",
        "➡ This leads to slow performance and higher memory usage.\n",
        "\n",
        "4. Sensitivity to Irrelevant Features\n",
        "\n",
        "Extra or irrelevant features add noise.\n",
        "\n",
        "These features dominate distance calculations.\n",
        "\n",
        "➡ Important features lose influence, reducing model accuracy."
      ],
      "metadata": {
        "id": "GZVhcc-Jw1hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3:What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans:Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much variance (information) as possible.\n",
        "\n",
        "PCA transforms the original correlated features into a new set of uncorrelated variables called principal components.\n",
        "\n",
        "Feature Selection\n",
        "\n",
        "Feature selection is the process of selecting a subset of original features that are most relevant to the prediction task.\n",
        "\n",
        "It does not create new features; it only keeps or removes existing ones.\n",
        "| Aspect                    | PCA                                         | Feature Selection                 |\n",
        "| ------------------------- | ------------------------------------------- | --------------------------------- |\n",
        "| Type                      | Feature extraction                          | Feature selection                 |\n",
        "| Supervised/Unsupervised   | Unsupervised                                | Can be supervised or unsupervised |\n",
        "| Feature creation          | Creates new features (principal components) | Uses original features            |\n",
        "| Interpretability          | Low (components are combinations)           | High (original features)          |\n",
        "| Goal                      | Maximize variance                           | Improve model performance         |\n",
        "| Handles multicollinearity | Yes                                         | Partially                         |\n"
      ],
      "metadata": {
        "id": "p9L8y5VwxeKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans:Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are mathematical concepts used to identify the most important directions in the data. They help PCA decide which features to keep and which to discard.\n",
        "\n",
        "Eigenvectors\n",
        "\n",
        "Eigenvectors represent the directions (axes) along which the data varies the most.\n",
        "\n",
        "In PCA, each eigenvector corresponds to a principal component.\n",
        "\n",
        "They are linear combinations of the original features.\n",
        "\n",
        "Eigenvectors determine the orientation of the new feature space.\n",
        "\n",
        "➡ Interpretation:\n",
        "An eigenvector shows where the data is spread the most.\n",
        "\n",
        "Eigenvalues\n",
        "\n",
        "Eigenvalues represent the amount of variance carried in the direction of their corresponding eigenvectors.\n",
        "\n",
        "A larger eigenvalue means more information (variance) is captured.\n",
        "\n",
        "Eigenvalues help rank principal components in order of importance.\n",
        "\n",
        "➡ Interpretation:\n",
        "An eigenvalue shows how important its eigenvector is.\n",
        "\n",
        "Role of Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "PCA computes the covariance matrix of the data.\n",
        "\n",
        "Eigenvectors and eigenvalues are calculated from this matrix.\n",
        "\n",
        "Eigenvectors with highest eigenvalues are selected.\n",
        "\n",
        "These selected eigenvectors form the principal components.\n",
        "\n",
        "Data is projected onto these components for dimensionality reduction.\n",
        "\n",
        "Why Are They Important in PCA?\n",
        "\n",
        "Help identify the most informative directions in data\n",
        "\n",
        "Enable dimensionality reduction with minimal information loss\n",
        "\n",
        "Reduce noise and redundancy\n",
        "\n",
        "Improve model efficiency and performance\n",
        "\n",
        "Remove multicollinearity among features\n",
        "\n",
        "Simple Example\n",
        "\n",
        "If PCA produces:\n",
        "\n",
        "Eigenvalue₁ = 5.2\n",
        "\n",
        "Eigenvalue₂ = 1.1\n",
        "\n",
        "Eigenvalue₃ = 0.2\n",
        "\n",
        "➡ The first principal component (Eigenvalue₁) explains the most variance, so it is kept."
      ],
      "metadata": {
        "id": "RyVeXZnxzIQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans:KNN and PCA are often used together because PCA helps overcome many limitations of KNN. When combined in a single pipeline, they improve accuracy, efficiency, and robustness.\n",
        "\n",
        "Role of PCA in the KNN Pipeline\n",
        "\n",
        "KNN is a distance-based algorithm, so its performance depends heavily on:\n",
        "\n",
        "Number of features\n",
        "\n",
        "Feature scale\n",
        "\n",
        "Noise and irrelevant variables\n",
        "\n",
        "PCA addresses these issues before KNN is applied.\n",
        "\n",
        "Step-by-Step KNN + PCA Pipeline\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Encode categorical variables\n",
        "\n",
        "Standardize features (important for both PCA and KNN)\n",
        "\n",
        "Apply PCA\n",
        "\n",
        "Reduce dimensionality\n",
        "\n",
        "Remove correlated and noisy features\n",
        "\n",
        "Retain components that explain most variance\n",
        "\n",
        "Apply KNN\n",
        "\n",
        "Compute distances in the reduced feature space\n",
        "\n",
        "Find nearest neighbors\n",
        "\n",
        "Perform classification or regression\n",
        "\n",
        "Model Evaluation\n",
        "\n",
        "Measure accuracy, precision, recall, RMSE, etc."
      ],
      "metadata": {
        "id": "LYg56IcNzpWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 6:Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "p-1ka8rg0O-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans 6\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --------- KNN WITHOUT Feature Scaling ---------\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
        "\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# --------- KNN WITH Feature Scaling ---------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without feature scaling:\", accuracy_no_scale)\n",
        "print(\"Accuracy with feature scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ0kTxQwFVIQ",
        "outputId": "ad056a83-51ae-4a27-e269-d15c6e4c1df3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 0.7407407407407407\n",
            "Accuracy with feature scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 7:Train a PCA model on the Wine dataset and print the explained variance\n",
        "#ratio of each principal component\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {var:.4f}\")\n"
      ],
      "metadata": {
        "id": "tqCpzr1Bw0gV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e34b2e-a61a-400d-9b7b-a61d479c575d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "maG_GRGbvmHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7e44ed-07ce-4fb5-e3b9-37856a99b751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9629629629629629\n",
            "Accuracy on PCA-transformed dataset (2 components): 0.9814814814814815\n"
          ]
        }
      ],
      "source": [
        "#Ques 8:: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "#components). Compare the accuracy with the original dataset.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------ KNN on Original Scaled Data ------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ------------------ PCA Transformation (Top 2 Components) ------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA-transformed dataset (2 components):\", accuracy_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ques 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "#manhattan) on the scaled Wine dataset and compare the results.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------- KNN with Euclidean Distance ---------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# --------- KNN with Manhattan Distance ---------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8qftd5Ggf-",
        "outputId": "e86045cd-1894-402c-d611-71a977df5dfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9629629629629629\n",
            "Accuracy with Manhattan distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Distance Metric | Accuracy       |\n",
        "| --------------- | -------------- |\n",
        "| Euclidean       | Higher         |\n",
        "| Manhattan       | Slightly lower |\n"
      ],
      "metadata": {
        "id": "qdx0uoxjG63D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 10:: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        "\n",
        "Ans:Dimensionality Reduction and Classification Pipeline for Cancer Detection\n",
        "\n",
        "Gene expression datasets typically contain thousands of genes (features) but very few patient samples, which makes traditional models prone to overfitting. To handle this, I would use a PCA + KNN pipeline.\n",
        "\n",
        "1. Using PCA to Reduce Dimensionality\n",
        "\n",
        "Gene expression features are often highly correlated.\n",
        "\n",
        "PCA transforms the original gene features into a smaller set of uncorrelated principal components.\n",
        "\n",
        "These components capture the maximum biological variation in the data while removing noise and redundancy.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Normalize gene expression values (standardization).\n",
        "\n",
        "Apply PCA to the scaled data.\n",
        "\n",
        "Project samples into the reduced feature space.\n",
        "\n",
        "➡ This reduces model complexity and improves generalization.\n",
        "\n",
        "2. Deciding How Many Principal Components to Keep\n",
        "\n",
        "To choose the optimal number of components:\n",
        "\n",
        "Explained Variance Ratio\n",
        "\n",
        "Retain components that explain 90–95% of total variance.\n",
        "\n",
        "Scree Plot\n",
        "\n",
        "Look for the elbow point, where additional components add minimal information.\n",
        "\n",
        "Cross-Validation Performance\n",
        "\n",
        "Test different numbers of components and select the value that maximizes validation accuracy.\n",
        "\n",
        "➡ This ensures minimal information loss while preventing overfitting.\n",
        "\n",
        "3. Using KNN for Classification After PCA\n",
        "\n",
        "After PCA, data lies in a low-dimensional, noise-reduced space.\n",
        "\n",
        "KNN classifies patients based on similar gene expression patterns.\n",
        "\n",
        "Why KNN works well here:\n",
        "\n",
        "Non-parametric (no strong assumptions about data distribution)\n",
        "\n",
        "Effective in reduced dimensions\n",
        "\n",
        "Captures local similarities between patients\n",
        "\n",
        "Key choices:\n",
        "\n",
        "Use Euclidean distance\n",
        "\n",
        "Tune K using cross-validation\n",
        "\n",
        "Optionally use distance-weighted KNN\n",
        "\n",
        "4. Evaluating the Model\n",
        "\n",
        "Because biomedical data is sensitive, evaluation must be robust:\n",
        "\n",
        "Cross-validation (e.g., stratified k-fold)\n",
        "\n",
        "Accuracy for overall performance\n",
        "\n",
        "Precision, Recall, F1-score to handle class imbalance\n",
        "\n",
        "Confusion Matrix to understand misclassifications\n",
        "\n",
        "ROC-AUC for diagnostic reliability\n",
        "\n",
        "➡ These metrics ensure the model is clinically meaningful, not just accurate.\n",
        "\n",
        "5. Justifying This Pipeline to Stakeholders\n",
        "\n",
        "To stakeholders (doctors, researchers, management), I would explain:\n",
        "\n",
        "PCA reduces noise and redundancy, improving model reliability\n",
        "\n",
        "Lower risk of overfitting due to fewer features\n",
        "\n",
        "KNN provides interpretable decisions by comparing patients to similar cases\n",
        "\n",
        "Faster and more scalable than high-dimensional models\n",
        "\n",
        "Clinically robust due to strong validation strategies\n",
        "\n",
        "Business & Clinical Value\n",
        "\n",
        "More reliable predictions\n",
        "\n",
        "Better generalization to new patients\n",
        "\n",
        "Reduced false diagnoses\n",
        "\n",
        "Supports data-driven medical decision-making\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Using PCA + KNN creates a robust, efficient, and interpretable pipeline for high-dimensional gene expression data. It balances statistical rigor with clinical practicality, making it suitable for real-world biomedical applications.\n",
        "\n",
        "“PCA mitigates overfitting by reducing dimensionality, while KNN leverages similarity in gene expression patterns for accurate cancer classification.”"
      ],
      "metadata": {
        "id": "REVc2X4CH2Br"
      }
    }
  ]
}